{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/madao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/madao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/madao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from fastai.text.all import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from gensim import models\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data\n",
    "The two datasets that we used for this project were found from Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.448296,
     "end_time": "2021-03-16T20:41:05.803133",
     "exception": false,
     "start_time": "2021-03-16T20:41:05.354837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path(f'{os.getcwd()}/../datasets/')\n",
    "tweets_covid_all_vaccination = pd.read_csv(path/'tweets_covid_all_vaccination.csv')\n",
    "tweets_extraction = pd.read_csv(path/'tweets_extraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.789357,
     "end_time": "2021-03-16T20:41:07.234895",
     "exception": false,
     "start_time": "2021-03-16T20:41:06.445538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove Emojis Helper\n",
    "def remove_emojis(str):\n",
    "    return str.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "# Remove URLs, Hashtags, handles, and Emojis\n",
    "def remove(ts, idx='text'):\n",
    "    ts['orig_text'] = ts[idx]\n",
    "    ts[idx] = ts[idx].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "    ts[idx] = ts[idx].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    ts[idx] = ts[idx].apply(remove_emojis)\n",
    "    ts[idx] = ts[idx].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "    return ts[ts[idx]!='']\n",
    "\n",
    "tweets_covid_all_vaccination['sentiment'] = np.nan\n",
    "tweets_covid_all_vaccination = remove(tweets_covid_all_vaccination)\n",
    "tweets_extraction = tweets_extraction[['old_text', 'new_sentiment']].rename(columns={'old_text':'text', 'new_sentiment':'sentiment'})\n",
    "tweets_extraction = remove(tweets_extraction)\n",
    "tweets_merge = tweets_extraction[['text', 'sentiment']].append(tweets_covid_all_vaccination[['text', 'sentiment']])\n",
    "tweets = tweets_merge.dropna(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.06579,
     "end_time": "2021-03-16T20:41:07.352724",
     "exception": false,
     "start_time": "2021-03-16T20:41:07.286934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We want to trade with someone who has Houston tickets, but no one will.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Re-pinging  why didn't you go to prom? BC my bf didn't like my friends</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31323 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              text  \\\n",
       "1                                                                     Layin n bed with a headache  ughhhh...waitin on your call...   \n",
       "2                                                                                              Funeral ceremony...gloomy friday...   \n",
       "3                                                                                             wants to hang out with friends SOON!   \n",
       "4                                                          We want to trade with someone who has Houston tickets, but no one will.   \n",
       "5                                                           Re-pinging  why didn't you go to prom? BC my bf didn't like my friends   \n",
       "...                                                                                                                            ...   \n",
       "39994                                                                                                Succesfully following Tayla!!   \n",
       "39996                                                                                               Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!   \n",
       "39998             WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!   \n",
       "39999     bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla   \n",
       "\n",
       "      sentiment  \n",
       "1      negative  \n",
       "2      negative  \n",
       "3      positive  \n",
       "4       neutral  \n",
       "5      negative  \n",
       "...         ...  \n",
       "39994  positive  \n",
       "39996  positive  \n",
       "39997  positive  \n",
       "39998   neutral  \n",
       "39999   neutral  \n",
       "\n",
       "[31323 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madao/anaconda3/envs/CS410/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tweets['sentiment'] = tweets['sentiment'].apply(lambda x: np.where(tweets['sentiment'].unique()== x )[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We want to trade with someone who has Houston tickets, but no one will.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Re-pinging  why didn't you go to prom? BC my bf didn't like my friends</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31323 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              text  \\\n",
       "1                                                                     Layin n bed with a headache  ughhhh...waitin on your call...   \n",
       "2                                                                                              Funeral ceremony...gloomy friday...   \n",
       "3                                                                                             wants to hang out with friends SOON!   \n",
       "4                                                          We want to trade with someone who has Houston tickets, but no one will.   \n",
       "5                                                           Re-pinging  why didn't you go to prom? BC my bf didn't like my friends   \n",
       "...                                                                                                                            ...   \n",
       "39994                                                                                                Succesfully following Tayla!!   \n",
       "39996                                                                                               Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!   \n",
       "39998             WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!   \n",
       "39999     bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla   \n",
       "\n",
       "       sentiment  \n",
       "1              0  \n",
       "2              0  \n",
       "3              1  \n",
       "4              2  \n",
       "5              0  \n",
       "...          ...  \n",
       "39994          1  \n",
       "39996          1  \n",
       "39997          1  \n",
       "39998          2  \n",
       "39999          2  \n",
       "\n",
       "[31323 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31323,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"text\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                 Layin n bed with a headache  ughhhh...waitin on your call...\n",
       "2                                          Funeral ceremony...gloomy friday...\n",
       "3                                         wants to hang out with friends SOON!\n",
       "4      We want to trade with someone who has Houston tickets, but no one will.\n",
       "5       Re-pinging  why didn't you go to prom? BC my bf didn't like my friends\n",
       "7                                                               Hmmm.  is down\n",
       "8                                                 Charlene my love. I miss you\n",
       "9                                             I'm sorry  at least it's Friday?\n",
       "10                                                            cant fall asleep\n",
       "11                                                     Choked on her retainers\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['layin', 'bed', 'headach', 'ughhhh', 'waitin', 'call'], ['funer', 'ceremoni', 'gloomi', 'friday'], ['want', 'hang', 'friend', 'soon'], ['want', 'trade', 'someon', 'houston', 'ticket', 'one'], ['reping', 'nt', 'go', 'prom', 'bc', 'bf', 'nt', 'like', 'friend'], ['hmmm'], ['charlen', 'love', 'miss'], ['sorri', 'least', 'friday'], ['cant', 'fall', 'asleep'], ['choke', 'retain']]\n"
     ]
    }
   ],
   "source": [
    "all_text_tokens = []\n",
    "for sentence in tweets[\"text\"]:\n",
    "    content_token = nltk.word_tokenize(sentence)\n",
    "\n",
    "    lower_token = []\n",
    "    for token in content_token:\n",
    "        lower_token.append(token.lower())\n",
    "\n",
    "    punctuation_token = []\n",
    "    for token in lower_token:\n",
    "        punctuation_token.append(re.sub(r'[^\\w\\s]+', '', token))\n",
    "\n",
    "    small_token = []\n",
    "    for token in punctuation_token:\n",
    "        if len(token)>1:\n",
    "            small_token.append(token)\n",
    "\n",
    "    stop_token = []\n",
    "    for token in small_token:\n",
    "        if token not in stopwords_list:\n",
    "            stop_token.append(token)\n",
    "\n",
    "    lemmatization_token = []\n",
    "    for token in stop_token:\n",
    "        lemmatization_token.append(WordNetLemmatizer().lemmatize(token))\n",
    "\n",
    "    stemming_token = []\n",
    "    for token in lemmatization_token:\n",
    "        stemming_token.append(PorterStemmer().stem(token))\n",
    "    \n",
    "    all_text_tokens.append(stemming_token)\n",
    "print(all_text_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madao/anaconda3/envs/CS410/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/madao/anaconda3/envs/CS410/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>\n",
       "      <td>0</td>\n",
       "      <td>layin bed headach ughhhh waitin call</td>\n",
       "      <td>[layin, bed, headach, ughhhh, waitin, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>0</td>\n",
       "      <td>funer ceremoni gloomi friday</td>\n",
       "      <td>[funer, ceremoni, gloomi, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>1</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We want to trade with someone who has Houston tickets, but no one will.</td>\n",
       "      <td>2</td>\n",
       "      <td>want trade someon houston ticket one</td>\n",
       "      <td>[want, trade, someon, houston, ticket, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Re-pinging  why didn't you go to prom? BC my bf didn't like my friends</td>\n",
       "      <td>0</td>\n",
       "      <td>reping nt go prom bc bf nt like friend</td>\n",
       "      <td>[reping, nt, go, prom, bc, bf, nt, like, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>1</td>\n",
       "      <td>succes follow tayla</td>\n",
       "      <td>[succes, follow, tayla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>1</td>\n",
       "      <td>happi mother day love</td>\n",
       "      <td>[happi, mother, day, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!</td>\n",
       "      <td>1</td>\n",
       "      <td>happi mother day mommi woman man long momma someon day</td>\n",
       "      <td>[happi, mother, day, mommi, woman, man, long, momma, someon, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!</td>\n",
       "      <td>2</td>\n",
       "      <td>wassup beauti follow peep new hit singl wwwmyspacecomipsohot def wat video</td>\n",
       "      <td>[wassup, beauti, follow, peep, new, hit, singl, wwwmyspacecomipsohot, def, wat, video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla</td>\n",
       "      <td>2</td>\n",
       "      <td>bullet train tokyo gf visit japan sinc thursday vacationsightse gaijin godzilla</td>\n",
       "      <td>[bullet, train, tokyo, gf, visit, japan, sinc, thursday, vacationsightse, gaijin, godzilla]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31323 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              text  \\\n",
       "1                                                                     Layin n bed with a headache  ughhhh...waitin on your call...   \n",
       "2                                                                                              Funeral ceremony...gloomy friday...   \n",
       "3                                                                                             wants to hang out with friends SOON!   \n",
       "4                                                          We want to trade with someone who has Houston tickets, but no one will.   \n",
       "5                                                           Re-pinging  why didn't you go to prom? BC my bf didn't like my friends   \n",
       "...                                                                                                                            ...   \n",
       "39994                                                                                                Succesfully following Tayla!!   \n",
       "39996                                                                                               Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!   \n",
       "39998             WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!   \n",
       "39999     bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla   \n",
       "\n",
       "       sentiment  \\\n",
       "1              0   \n",
       "2              0   \n",
       "3              1   \n",
       "4              2   \n",
       "5              0   \n",
       "...          ...   \n",
       "39994          1   \n",
       "39996          1   \n",
       "39997          1   \n",
       "39998          2   \n",
       "39999          2   \n",
       "\n",
       "                                                                            text_final  \\\n",
       "1                                                 layin bed headach ughhhh waitin call   \n",
       "2                                                         funer ceremoni gloomi friday   \n",
       "3                                                                want hang friend soon   \n",
       "4                                                 want trade someon houston ticket one   \n",
       "5                                               reping nt go prom bc bf nt like friend   \n",
       "...                                                                                ...   \n",
       "39994                                                              succes follow tayla   \n",
       "39996                                                            happi mother day love   \n",
       "39997                           happi mother day mommi woman man long momma someon day   \n",
       "39998       wassup beauti follow peep new hit singl wwwmyspacecomipsohot def wat video   \n",
       "39999  bullet train tokyo gf visit japan sinc thursday vacationsightse gaijin godzilla   \n",
       "\n",
       "                                                                                       text_tokens  \n",
       "1                                                      [layin, bed, headach, ughhhh, waitin, call]  \n",
       "2                                                                [funer, ceremoni, gloomi, friday]  \n",
       "3                                                                       [want, hang, friend, soon]  \n",
       "4                                                      [want, trade, someon, houston, ticket, one]  \n",
       "5                                                 [reping, nt, go, prom, bc, bf, nt, like, friend]  \n",
       "...                                                                                            ...  \n",
       "39994                                                                      [succes, follow, tayla]  \n",
       "39996                                                                   [happi, mother, day, love]  \n",
       "39997                            [happi, mother, day, mommi, woman, man, long, momma, someon, day]  \n",
       "39998       [wassup, beauti, follow, peep, new, hit, singl, wwwmyspacecomipsohot, def, wat, video]  \n",
       "39999  [bullet, train, tokyo, gf, visit, japan, sinc, thursday, vacationsightse, gaijin, godzilla]  \n",
       "\n",
       "[31323 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text_final'] = [' '.join(text) for text in all_text_tokens]\n",
    "tweets['text_tokens'] = all_text_tokens\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vaccine Tweets Sentiment Analysis\n",
    "## 1) Baseline Model - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets[\"text_final\"]\n",
    "y = tweets[\"sentiment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25058,), (6265,), (25058,), (6265,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_nb = TfidfVectorizer(stop_words = stopwords.words('english')).fit(X)\n",
    "X_train_nb = vector_nb.transform(X_train)\n",
    "X_test_nb = vector_nb.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_naive_bayes = MultinomialNB()\n",
    "my_naive_bayes.fit(X_train_nb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuary: 0.6260175578611333\n"
     ]
    }
   ],
   "source": [
    "pred = my_naive_bayes.predict(X_test_nb)\n",
    "accuracy_score = metrics.accuracy_score(pred, y_test)\n",
    "print(f\"Naive Bayes accuary: {accuracy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Baseline Model - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:30:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost train accuary: 0.76678106792242\n",
      "XGBoost test accuary: 0.6916201117318436\n"
     ]
    }
   ],
   "source": [
    "vector1 = TfidfVectorizer(stop_words = stopwords.words('english')).fit(X)\n",
    "X_train1 = vector1.transform(X_train)\n",
    "X_test1 = vector1.transform(X_test)\n",
    "\n",
    "XGB_model_1 = XGBClassifier(random_state=410)\n",
    "XGB_model_1.fit(X_train1, y_train)\n",
    "\n",
    "print(f\"XGBoost train accuary: {XGB_model_1.score(X_train1, y_train)}\")\n",
    "print(f\"XGBoost test accuary: {XGB_model_1.score(X_test1, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:30:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost train accuary: 0.7686567164179104\n",
      "XGBoost test accuary: 0.6927374301675978\n"
     ]
    }
   ],
   "source": [
    "vector2 = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range=(1, 2)).fit(X)\n",
    "X_train2 = vector2.transform(X_train)\n",
    "X_test2 = vector2.transform(X_test)\n",
    "\n",
    "XGB_model_2 = XGBClassifier(random_state=410)\n",
    "XGB_model_2.fit(X_train2, y_train)\n",
    "\n",
    "print(f\"XGBoost train accuary: {XGB_model_2.score(X_train2, y_train)}\")\n",
    "print(f\"XGBoost test accuary: {XGB_model_2.score(X_test2, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:30:49] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost train accuary: 0.7692154202250778\n",
      "XGBoost test accuary: 0.6964086193136473\n"
     ]
    }
   ],
   "source": [
    "vector3 = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range=(1, 3)).fit(X)\n",
    "X_train3 = vector3.transform(X_train)\n",
    "X_test3 = vector3.transform(X_test)\n",
    "\n",
    "XGB_model_3 = XGBClassifier(random_state=410)\n",
    "XGB_model_3.fit(X_train3, y_train)\n",
    "\n",
    "print(f\"XGBoost train accuary: {XGB_model_3.score(X_train3, y_train)}\")\n",
    "print(f\"XGBoost test accuary: {XGB_model_3.score(X_test3, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:31:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost train accuary: 0.76797829036635\n",
      "XGBoost test accuary: 0.6951316839584996\n"
     ]
    }
   ],
   "source": [
    "vector4 = TfidfVectorizer(stop_words = stopwords.words('english'), ngram_range=(1, 4)).fit(X)\n",
    "X_train4 = vector4.transform(X_train)\n",
    "X_test4 = vector4.transform(X_test)\n",
    "\n",
    "XGB_model_4 = XGBClassifier(random_state=410)\n",
    "XGB_model_4.fit(X_train4, y_train)\n",
    "\n",
    "print(f\"XGBoost train accuary: {XGB_model_4.score(X_train4, y_train)}\")\n",
    "print(f\"XGBoost test accuary: {XGB_model_4.score(X_test4, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Deep-Learning-based Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madao/anaconda3/envs/CS410/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/madao/anaconda3/envs/CS410/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/madao/anaconda3/envs/CS410/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>netrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>\n",
       "      <td>0</td>\n",
       "      <td>layin bed headach ughhhh waitin call</td>\n",
       "      <td>[layin, bed, headach, ughhhh, waitin, call]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>0</td>\n",
       "      <td>funer ceremoni gloomi friday</td>\n",
       "      <td>[funer, ceremoni, gloomi, friday]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>1</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We want to trade with someone who has Houston tickets, but no one will.</td>\n",
       "      <td>2</td>\n",
       "      <td>want trade someon houston ticket one</td>\n",
       "      <td>[want, trade, someon, houston, ticket, one]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Re-pinging  why didn't you go to prom? BC my bf didn't like my friends</td>\n",
       "      <td>0</td>\n",
       "      <td>reping nt go prom bc bf nt like friend</td>\n",
       "      <td>[reping, nt, go, prom, bc, bf, nt, like, friend]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>1</td>\n",
       "      <td>succes follow tayla</td>\n",
       "      <td>[succes, follow, tayla]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>1</td>\n",
       "      <td>happi mother day love</td>\n",
       "      <td>[happi, mother, day, love]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!</td>\n",
       "      <td>1</td>\n",
       "      <td>happi mother day mommi woman man long momma someon day</td>\n",
       "      <td>[happi, mother, day, mommi, woman, man, long, momma, someon, day]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!</td>\n",
       "      <td>2</td>\n",
       "      <td>wassup beauti follow peep new hit singl wwwmyspacecomipsohot def wat video</td>\n",
       "      <td>[wassup, beauti, follow, peep, new, hit, singl, wwwmyspacecomipsohot, def, wat, video]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla</td>\n",
       "      <td>2</td>\n",
       "      <td>bullet train tokyo gf visit japan sinc thursday vacationsightse gaijin godzilla</td>\n",
       "      <td>[bullet, train, tokyo, gf, visit, japan, sinc, thursday, vacationsightse, gaijin, godzilla]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31323 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              text  \\\n",
       "1                                                                     Layin n bed with a headache  ughhhh...waitin on your call...   \n",
       "2                                                                                              Funeral ceremony...gloomy friday...   \n",
       "3                                                                                             wants to hang out with friends SOON!   \n",
       "4                                                          We want to trade with someone who has Houston tickets, but no one will.   \n",
       "5                                                           Re-pinging  why didn't you go to prom? BC my bf didn't like my friends   \n",
       "...                                                                                                                            ...   \n",
       "39994                                                                                                Succesfully following Tayla!!   \n",
       "39996                                                                                               Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out there, be you woman or man as long as you're 'momma' to someone this is your day!   \n",
       "39998             WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEEP OUT MY NEW HIT SINGLES WWW.MYSPACE.COM/IPSOHOT I DEF. WAT U IN THE VIDEO!!   \n",
       "39999     bullet train from tokyo    the gf and i have been visiting japan since thursday  vacation/sightseeing    gaijin godzilla   \n",
       "\n",
       "       sentiment  \\\n",
       "1              0   \n",
       "2              0   \n",
       "3              1   \n",
       "4              2   \n",
       "5              0   \n",
       "...          ...   \n",
       "39994          1   \n",
       "39996          1   \n",
       "39997          1   \n",
       "39998          2   \n",
       "39999          2   \n",
       "\n",
       "                                                                            text_final  \\\n",
       "1                                                 layin bed headach ughhhh waitin call   \n",
       "2                                                         funer ceremoni gloomi friday   \n",
       "3                                                                want hang friend soon   \n",
       "4                                                 want trade someon houston ticket one   \n",
       "5                                               reping nt go prom bc bf nt like friend   \n",
       "...                                                                                ...   \n",
       "39994                                                              succes follow tayla   \n",
       "39996                                                            happi mother day love   \n",
       "39997                           happi mother day mommi woman man long momma someon day   \n",
       "39998       wassup beauti follow peep new hit singl wwwmyspacecomipsohot def wat video   \n",
       "39999  bullet train tokyo gf visit japan sinc thursday vacationsightse gaijin godzilla   \n",
       "\n",
       "                                                                                       text_tokens  \\\n",
       "1                                                      [layin, bed, headach, ughhhh, waitin, call]   \n",
       "2                                                                [funer, ceremoni, gloomi, friday]   \n",
       "3                                                                       [want, hang, friend, soon]   \n",
       "4                                                      [want, trade, someon, houston, ticket, one]   \n",
       "5                                                 [reping, nt, go, prom, bc, bf, nt, like, friend]   \n",
       "...                                                                                            ...   \n",
       "39994                                                                      [succes, follow, tayla]   \n",
       "39996                                                                   [happi, mother, day, love]   \n",
       "39997                            [happi, mother, day, mommi, woman, man, long, momma, someon, day]   \n",
       "39998       [wassup, beauti, follow, peep, new, hit, singl, wwwmyspacecomipsohot, def, wat, video]   \n",
       "39999  [bullet, train, tokyo, gf, visit, japan, sinc, thursday, vacationsightse, gaijin, godzilla]   \n",
       "\n",
       "       neg  pos  netrl  \n",
       "1        1    0      0  \n",
       "2        1    0      0  \n",
       "3        0    1      0  \n",
       "4        0    0      1  \n",
       "5        1    0      0  \n",
       "...    ...  ...    ...  \n",
       "39994    0    1      0  \n",
       "39996    0    1      0  \n",
       "39997    0    1      0  \n",
       "39998    0    0      1  \n",
       "39999    0    0      1  \n",
       "\n",
       "[31323 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = []\n",
    "pos = []\n",
    "netrl = []\n",
    "\n",
    "for senti in tweets['sentiment']:\n",
    "    # negative\n",
    "    if senti == 0:\n",
    "        neg.append(1)\n",
    "        pos.append(0)\n",
    "        netrl.append(0)\n",
    "    # positive\n",
    "    elif senti == 1:\n",
    "        neg.append(0)\n",
    "        pos.append(1)\n",
    "        netrl.append(0)    \n",
    "    # neutral\n",
    "    else:\n",
    "        neg.append(0)\n",
    "        pos.append(0)\n",
    "        netrl.append(1)\n",
    "        \n",
    "tweets['neg']= neg\n",
    "tweets['pos']= pos\n",
    "tweets['netrl'] = netrl\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>netrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>layin bed headach ughhhh waitin call</td>\n",
       "      <td>[layin, bed, headach, ughhhh, waitin, call]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funer ceremoni gloomi friday</td>\n",
       "      <td>[funer, ceremoni, gloomi, friday]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>[want, hang, friend, soon]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want trade someon houston ticket one</td>\n",
       "      <td>[want, trade, someon, houston, ticket, one]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reping nt go prom bc bf nt like friend</td>\n",
       "      <td>[reping, nt, go, prom, bc, bf, nt, like, friend]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text_final  \\\n",
       "1    layin bed headach ughhhh waitin call   \n",
       "2            funer ceremoni gloomi friday   \n",
       "3                   want hang friend soon   \n",
       "4    want trade someon houston ticket one   \n",
       "5  reping nt go prom bc bf nt like friend   \n",
       "\n",
       "                                        text_tokens  sentiment  neg  pos  \\\n",
       "1       [layin, bed, headach, ughhhh, waitin, call]          0    1    0   \n",
       "2                 [funer, ceremoni, gloomi, friday]          0    1    0   \n",
       "3                        [want, hang, friend, soon]          1    0    1   \n",
       "4       [want, trade, someon, houston, ticket, one]          2    0    0   \n",
       "5  [reping, nt, go, prom, bc, bf, nt, like, friend]          0    1    0   \n",
       "\n",
       "   netrl  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      1  \n",
       "5      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tweets[['text_final', 'text_tokens', 'sentiment', 'neg', 'pos', 'netrl']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223166 train words\n",
      "21290 train vocabularies\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://notebook.community/rahulavadhoot/Portfolio/projects/natural%20language%20processing/Disasters%20on%20social%20media/Disasters%20on%20social%20media\n",
    "train_words = [word for tokens in data[\"text_tokens\"] for word in tokens]\n",
    "train_vocs = sorted(list(set(train_words)))\n",
    "print(str(len(train_words)) + \" train words\")\n",
    "print(str(len(train_vocs)) + \" train vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = models.KeyedVectors.load_word2vec_format('/home/madao/CS410/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['text_tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = get_word2vec_embeddings(word2vec, data, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21270 unique tokens after vectorization\n"
     ]
    }
   ],
   "source": [
    "text_final_list = data[\"text_final\"].tolist()\n",
    "tokenizer = Tokenizer(num_words=len(train_vocs), char_level=False)\n",
    "tokenizer.fit_on_texts(text_final_list)\n",
    "train_sequences = tokenizer.texts_to_sequences(text_final_list)\n",
    "train_tokens = tokenizer.word_index\n",
    "print(str(len(train_tokens)) + \" unique tokens after vectorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 65\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21271, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_tokens)+1, EMBEDDING_DIM))\n",
    "for word,index in train_tokens.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data[['neg', 'pos', 'netrl']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train\n",
    "print(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 65)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 65, 300)      6381300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 64, 200)      120200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 63, 200)      180200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 62, 200)      240200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 61, 200)      300200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 60, 200)      360200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,710,815\n",
      "Trainable params: 1,329,515\n",
      "Non-trainable params: 6,381,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_tokens)+1, EMBEDDING_DIM, len(list(['neg', 'pos', 'netrl'])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "314/314 [==============================] - 69s 215ms/step - loss: 0.5224 - acc: 0.5947 - val_loss: 0.4174 - val_acc: 0.7117\n",
      "Epoch 2/5\n",
      "314/314 [==============================] - 68s 216ms/step - loss: 0.4336 - acc: 0.6874 - val_loss: 0.3849 - val_acc: 0.7401\n",
      "Epoch 3/5\n",
      "314/314 [==============================] - 67s 213ms/step - loss: 0.3920 - acc: 0.7286 - val_loss: 0.4162 - val_acc: 0.7058\n",
      "Epoch 4/5\n",
      "314/314 [==============================] - 70s 222ms/step - loss: 0.3327 - acc: 0.7780 - val_loss: 0.4044 - val_acc: 0.7251\n",
      "Epoch 5/5\n",
      "314/314 [==============================] - 69s 218ms/step - loss: 0.2702 - acc: 0.8249 - val_loss: 0.4384 - val_acc: 0.7176\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 80\n",
    "test = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.2, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1046.164914,
   "end_time": "2021-03-16T20:58:18.937939",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-16T20:40:52.773025",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
